{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dada910",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c34ff7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bd66e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\AppData\\Local\\Temp\\ipykernel_4612\\3554030502.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(FILE_PATH, parse_dates=['Date'], index_col='Date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded. Filtering for dates between 2019-02-01 and 2024-12-31.\n",
      "Original total rows: 8757, Rows after filtering: 1480\n",
      "\n",
      "--- Sample of Spot Rates (Filtered and in Decimals) ---\n",
      "              1 Yr    2 Yr    5 Yr   10 Yr   20 Yr   30 Yr\n",
      "Date                                                      \n",
      "2019-02-01  0.0256  0.0252  0.0251  0.0270  0.0288  0.0303\n",
      "2019-02-04  0.0257  0.0253  0.0253  0.0273  0.0292  0.0306\n",
      "2019-02-05  0.0256  0.0253  0.0251  0.0271  0.0289  0.0303\n",
      "2019-02-06  0.0256  0.0252  0.0250  0.0270  0.0288  0.0303\n",
      "2019-02-07  0.0255  0.0248  0.0246  0.0265  0.0285  0.0300\n",
      "...\n",
      "              1 Yr    2 Yr    5 Yr   10 Yr   20 Yr   30 Yr\n",
      "Date                                                      \n",
      "2024-12-24  0.0424  0.0429  0.0443  0.0459  0.0484  0.0476\n",
      "2024-12-26  0.0423  0.0430  0.0442  0.0458  0.0483  0.0476\n",
      "2024-12-27  0.0420  0.0431  0.0445  0.0462  0.0489  0.0482\n",
      "2024-12-30  0.0417  0.0424  0.0437  0.0455  0.0484  0.0477\n",
      "2024-12-31  0.0416  0.0425  0.0438  0.0458  0.0486  0.0478\n",
      "\n",
      "--- Sample of Calculated Forward Rates ---\n",
      "            f_1y_2y   f_2y_5y  f_5y_10y  f_10y_20y  f_20y_30y\n",
      "Date                                                         \n",
      "2024-12-24   0.0434  0.045233    0.0475     0.0509     0.0460\n",
      "2024-12-26   0.0437  0.045000    0.0474     0.0508     0.0462\n",
      "2024-12-27   0.0442  0.045433    0.0479     0.0516     0.0468\n",
      "2024-12-30   0.0431  0.044567    0.0473     0.0513     0.0463\n",
      "2024-12-31   0.0434  0.044667    0.0478     0.0514     0.0462\n",
      "\n",
      "--- Sample of Daily Changes Matrix (Final Output) ---\n",
      "                 f_1y_2y   f_2y_5y  f_5y_10y  f_10y_20y  f_20y_30y\n",
      "Date                                                              \n",
      "2024-12-24  6.938894e-18 -0.000100    0.0001    -0.0002    -0.0004\n",
      "2024-12-26  3.000000e-04 -0.000233   -0.0001    -0.0001     0.0002\n",
      "2024-12-27  5.000000e-04  0.000433    0.0005     0.0008     0.0006\n",
      "2024-12-30 -1.100000e-03 -0.000867   -0.0006    -0.0003    -0.0005\n",
      "2024-12-31  3.000000e-04  0.000100    0.0005     0.0001    -0.0001\n",
      "\n",
      "Processed matrix saved to: C:/Users/calvi/Thesis ALM/Thesis-ALM/Yield Curve Simulation/NSS/yield-curve-rates-1990-2024_forward_changes_matrix_2019-02-01_to_2024-12-31.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------- 1. CONFIGURATION -----------------\n",
    "# File path for the historical yield curve data\n",
    "FILE_PATH = r\"C:/Users/calvi/Thesis ALM/Thesis-ALM/Yield Curve Simulation/NSS/yield-curve-rates-1990-2024.csv\"\n",
    "\n",
    "# Select the fixed set of maturities (T) in years\n",
    "selected_maturities_yrs = [1, 2, 5, 10, 20, 30]\n",
    "\n",
    "# Define the date range for the analysis\n",
    "start_date = '2019-02-01'\n",
    "end_date = '2024-12-31' # The code will use data up to the last available date within this range\n",
    "\n",
    "# ----------------- 2. LOAD AND PREPARE DATA -----------------\n",
    "try:\n",
    "    # Load the entire dataset, parsing 'Date' as dates and setting it as the index\n",
    "    df = pd.read_csv(FILE_PATH, parse_dates=['Date'], index_col='Date')\n",
    "    \n",
    "    # Sort chronologically (Oldest -> Newest) is essential for time series operations\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # --- NEW: FILTER DATA BY DATE RANGE ---\n",
    "    # Use .loc to select all rows from the start_date to the end_date\n",
    "    df_filtered = df.loc[start_date:end_date]\n",
    "    print(f\"Data successfully loaded. Filtering for dates between {start_date} and {end_date}.\")\n",
    "    print(f\"Original total rows: {len(df)}, Rows after filtering: {len(df_filtered)}\")\n",
    "    \n",
    "    # Map integer maturities to column names (e.g., 1 -> '1 Yr')\n",
    "    col_map = {T: f'{T} Yr' for T in selected_maturities_yrs}\n",
    "    selected_columns = list(col_map.values())\n",
    "\n",
    "    # Check if all required columns exist in the file\n",
    "    missing_cols = [c for c in selected_columns if c not in df_filtered.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columns not found in CSV: {missing_cols}. Please check CSV headers.\")\n",
    "\n",
    "    # Create the spot rates DataFrame with only the selected maturities\n",
    "    spot_rates_df = df_filtered[selected_columns].copy()\n",
    "\n",
    "    # Drop any rows within our date range that have missing values for the selected tenors\n",
    "    original_count = len(spot_rates_df)\n",
    "    spot_rates_df.dropna(inplace=True)\n",
    "    cleaned_count = len(spot_rates_df)\n",
    "    \n",
    "    if cleaned_count < original_count:\n",
    "        print(f\"Note: Dropped {original_count - cleaned_count} rows from the selected period due to missing data.\")\n",
    "\n",
    "    # Convert rates from percentages to decimals (e.g., 4.5 -> 0.045)\n",
    "    spot_rates_df = spot_rates_df / 100\n",
    "\n",
    "    # ----------------- 3. DERIVE FORWARD RATES -----------------\n",
    "    forward_rates_df = pd.DataFrame(index=spot_rates_df.index)\n",
    "\n",
    "    # Loop through maturities to calculate forward rates between each consecutive pair (T1, T2)\n",
    "    for i in range(len(selected_maturities_yrs) - 1):\n",
    "        T1 = selected_maturities_yrs[i]\n",
    "        T2 = selected_maturities_yrs[i+1]\n",
    "        \n",
    "        col_T1 = col_map[T1]\n",
    "        col_T2 = col_map[T2]\n",
    "        \n",
    "        R1 = spot_rates_df[col_T1]\n",
    "        R2 = spot_rates_df[col_T2]\n",
    "        \n",
    "        # Formula: f(T1, T2) = (T2 * R(T2) - T1 * R(T1)) / (T2 - T1)\n",
    "        forward_rate = (T2 * R2 - T1 * R1) / (T2 - T1)\n",
    "        \n",
    "        # Use a descriptive column name for the forward rate\n",
    "        col_name = f'f_{T1}y_{T2}y'\n",
    "        forward_rates_df[col_name] = forward_rate\n",
    "\n",
    "    # ----------------- 4. CALCULATE DAILY CHANGES -----------------\n",
    "    # Use the .diff() method to calculate the simple daily change\n",
    "    daily_changes_df = forward_rates_df.diff()\n",
    "\n",
    "    # The first row after .diff() is always NaN, so drop it\n",
    "    final_matrix = daily_changes_df.dropna()\n",
    "\n",
    "    # ----------------- 5. VIEW AND SAVE RESULTS -----------------\n",
    "    print(\"\\n--- Sample of Spot Rates (Filtered and in Decimals) ---\")\n",
    "    print(spot_rates_df.head()) # Show the beginning of the filtered range\n",
    "    print(\"...\")\n",
    "    print(spot_rates_df.tail()) # Show the end\n",
    "\n",
    "    print(\"\\n--- Sample of Calculated Forward Rates ---\")\n",
    "    print(forward_rates_df.tail())\n",
    "\n",
    "    print(\"\\n--- Sample of Daily Changes Matrix (Final Output) ---\")\n",
    "    print(final_matrix.tail())\n",
    "\n",
    "    # OPTIONAL: Save the processed matrix to a new, descriptively named CSV\n",
    "    output_path = FILE_PATH.replace(\".csv\", f\"_forward_changes_matrix_{start_date}_to_{end_date}.csv\")\n",
    "    final_matrix.to_csv(output_path)\n",
    "    print(f\"\\nProcessed matrix saved to: {output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {FILE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc7c071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\AppData\\Local\\Temp\\ipykernel_4612\\2811947880.py:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(FILE_PATH, parse_dates=['Date'], index_col='Date')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded. Filtering for dates between 2019-02-01 and 2024-12-31.\n",
      "Original total rows: 8757, Rows after filtering: 1480\n",
      "\n",
      "--- Sample of Selected Spot Rates (Filtered and in Decimals) ---\n",
      "              1 Mo    3 Mo    6 Mo    1 Yr    2 Yr    5 Yr   10 Yr   20 Yr  \\\n",
      "Date                                                                         \n",
      "2024-12-24  0.0444  0.0440  0.0430  0.0424  0.0429  0.0443  0.0459  0.0484   \n",
      "2024-12-26  0.0445  0.0435  0.0431  0.0423  0.0430  0.0442  0.0458  0.0483   \n",
      "2024-12-27  0.0444  0.0431  0.0429  0.0420  0.0431  0.0445  0.0462  0.0489   \n",
      "2024-12-30  0.0443  0.0437  0.0425  0.0417  0.0424  0.0437  0.0455  0.0484   \n",
      "2024-12-31  0.0440  0.0437  0.0424  0.0416  0.0425  0.0438  0.0458  0.0486   \n",
      "\n",
      "             30 Yr  \n",
      "Date                \n",
      "2024-12-24  0.0476  \n",
      "2024-12-26  0.0476  \n",
      "2024-12-27  0.0482  \n",
      "2024-12-30  0.0477  \n",
      "2024-12-31  0.0478  \n",
      "\n",
      "--- Sample of Calculated Forward Rates ---\n",
      "            f_1m_3m  f_3m_6m  f_6m_1y  f_1y_2y   f_2y_5y  f_5y_10y  f_10y_20y  \\\n",
      "Date                                                                            \n",
      "2024-12-24  0.04380   0.0420   0.0418   0.0434  0.045233    0.0475     0.0509   \n",
      "2024-12-26  0.04300   0.0427   0.0415   0.0437  0.045000    0.0474     0.0508   \n",
      "2024-12-27  0.04245   0.0427   0.0411   0.0442  0.045433    0.0479     0.0516   \n",
      "2024-12-30  0.04340   0.0413   0.0409   0.0431  0.044567    0.0473     0.0513   \n",
      "2024-12-31  0.04355   0.0411   0.0408   0.0434  0.044667    0.0478     0.0514   \n",
      "\n",
      "            f_20y_30y  \n",
      "Date                   \n",
      "2024-12-24     0.0460  \n",
      "2024-12-26     0.0462  \n",
      "2024-12-27     0.0468  \n",
      "2024-12-30     0.0463  \n",
      "2024-12-31     0.0462  \n",
      "\n",
      "--- Sample of Daily Changes Matrix (Final Output) ---\n",
      "            f_1m_3m  f_3m_6m  f_6m_1y       f_1y_2y   f_2y_5y  f_5y_10y  \\\n",
      "Date                                                                      \n",
      "2024-12-24  0.00060  -0.0004  -0.0004  6.938894e-18 -0.000100    0.0001   \n",
      "2024-12-26 -0.00080   0.0007  -0.0003  3.000000e-04 -0.000233   -0.0001   \n",
      "2024-12-27 -0.00055   0.0000  -0.0004  5.000000e-04  0.000433    0.0005   \n",
      "2024-12-30  0.00095  -0.0014  -0.0002 -1.100000e-03 -0.000867   -0.0006   \n",
      "2024-12-31  0.00015  -0.0002  -0.0001  3.000000e-04  0.000100    0.0005   \n",
      "\n",
      "            f_10y_20y  f_20y_30y  \n",
      "Date                              \n",
      "2024-12-24    -0.0002    -0.0004  \n",
      "2024-12-26    -0.0001     0.0002  \n",
      "2024-12-27     0.0008     0.0006  \n",
      "2024-12-30    -0.0003    -0.0005  \n",
      "2024-12-31     0.0001    -0.0001  \n",
      "\n",
      "Processed matrix saved to: C:/Users/calvi/Thesis ALM/Thesis-ALM/Yield Curve Simulation/NSS/yield-curve-rates-1990-20241_forward_changes_matrix_2019-02-01_to_2024-12-31_with_months.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------- 1. CONFIGURATION -----------------\n",
    "# File path for the historical yield curve data\n",
    "FILE_PATH = r\"C:/Users/calvi/Thesis ALM/Thesis-ALM/Yield Curve Simulation/NSS/yield-curve-rates-1990-2024.csv\"\n",
    "\n",
    "# --- NEW: Define maturities with mixed units (Months and Years) ---\n",
    "# Each tuple contains (value, unit). This list must be in increasing order of maturity.\n",
    "selected_maturities = [ (1, 'Mo'),\n",
    "    (3, 'Mo'),\n",
    "    (6, 'Mo'),\n",
    "    (1, 'Yr'),\n",
    "    (2, 'Yr'),\n",
    "    (5, 'Yr'),\n",
    "    (10, 'Yr'),\n",
    "    (20, 'Yr'),\n",
    "    (30, 'Yr')\n",
    "]\n",
    "\n",
    "# Define the date range for the analysis\n",
    "start_date = '2019-02-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# ----------------- 2. LOAD AND PREPARE DATA -----------------\n",
    "\n",
    "# --- Helper structures to process mixed-unit maturities ---\n",
    "selected_columns = []      # e.g., ['3 Mo', '6 Mo', '1 Yr']\n",
    "maturities_in_years = {}   # e.g., {'3 Mo': 0.25, '1 Yr': 1.0}\n",
    "short_labels = []          # e.g., ['3m', '6m', '1y']\n",
    "\n",
    "for value, unit in selected_maturities:\n",
    "    # Create the column name that matches the CSV header\n",
    "    column_name = f'{value} {unit}'\n",
    "    selected_columns.append(column_name)\n",
    "    \n",
    "    # Create a short label for naming forward rate columns\n",
    "    short_label = f'{value}m' if unit == 'Mo' else f'{value}y'\n",
    "    short_labels.append(short_label)\n",
    "    \n",
    "    # CRITICAL: Convert all maturities to a consistent unit (years) for the formula\n",
    "    year_value = value / 12 if unit == 'Mo' else float(value)\n",
    "    maturities_in_years[column_name] = year_value\n",
    "\n",
    "try:\n",
    "    # Load the entire dataset\n",
    "    df = pd.read_csv(FILE_PATH, parse_dates=['Date'], index_col='Date')\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Filter data by the specified date range\n",
    "    df_filtered = df.loc[start_date:end_date]\n",
    "    print(f\"Data successfully loaded. Filtering for dates between {start_date} and {end_date}.\")\n",
    "    print(f\"Original total rows: {len(df)}, Rows after filtering: {len(df_filtered)}\")\n",
    "    \n",
    "    # Check if all required columns exist in the file\n",
    "    missing_cols = [c for c in selected_columns if c not in df_filtered.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Columns not found in CSV: {missing_cols}. Please check CSV headers.\")\n",
    "\n",
    "    # Create the spot rates DataFrame\n",
    "    spot_rates_df = df_filtered[selected_columns].copy()\n",
    "\n",
    "    # Drop rows with any missing values for the selected tenors\n",
    "    original_count = len(spot_rates_df)\n",
    "    spot_rates_df.dropna(inplace=True)\n",
    "    cleaned_count = len(spot_rates_df)\n",
    "    \n",
    "    if cleaned_count < original_count:\n",
    "        print(f\"Note: Dropped {original_count - cleaned_count} rows from the selected period due to missing data.\")\n",
    "\n",
    "    # Convert rates from percentages to decimals\n",
    "    spot_rates_df = spot_rates_df / 100\n",
    "\n",
    "    # ----------------- 3. DERIVE FORWARD RATES -----------------\n",
    "    forward_rates_df = pd.DataFrame(index=spot_rates_df.index)\n",
    "\n",
    "    # Loop through maturities to calculate forward rates between each consecutive pair\n",
    "    for i in range(len(selected_columns) - 1):\n",
    "        col_T1 = selected_columns[i]\n",
    "        col_T2 = selected_columns[i+1]\n",
    "        \n",
    "        # Get the maturity values IN YEARS from our helper dictionary\n",
    "        T1 = maturities_in_years[col_T1]\n",
    "        T2 = maturities_in_years[col_T2]\n",
    "        \n",
    "        # Get the spot rate series\n",
    "        R1 = spot_rates_df[col_T1]\n",
    "        R2 = spot_rates_df[col_T2]\n",
    "        \n",
    "        # Apply the forward rate formula using the converted year values\n",
    "        forward_rate = (T2 * R2 - T1 * R1) / (T2 - T1)\n",
    "        \n",
    "        # Use the short labels for a clean column name (e.g., 'f_3m_6m', 'f_6m_1y')\n",
    "        label1 = short_labels[i]\n",
    "        label2 = short_labels[i+1]\n",
    "        col_name = f'f_{label1}_{label2}'\n",
    "        forward_rates_df[col_name] = forward_rate\n",
    "\n",
    "    # ----------------- 4. CALCULATE DAILY CHANGES -----------------\n",
    "    daily_changes_df = forward_rates_df.diff()\n",
    "    final_matrix = daily_changes_df.dropna()\n",
    "\n",
    "    # ----------------- 5. VIEW AND SAVE RESULTS -----------------\n",
    "    print(\"\\n--- Sample of Selected Spot Rates (Filtered and in Decimals) ---\")\n",
    "    print(spot_rates_df.tail())\n",
    "\n",
    "    print(\"\\n--- Sample of Calculated Forward Rates ---\")\n",
    "    print(forward_rates_df.tail())\n",
    "\n",
    "    print(\"\\n--- Sample of Daily Changes Matrix (Final Output) ---\")\n",
    "    print(final_matrix.tail())\n",
    "\n",
    "    # Save the processed matrix to a new CSV\n",
    "    output_path = FILE_PATH.replace(\".csv\", f\"1_forward_changes_matrix_{start_date}_to_{end_date}_with_months.csv\")\n",
    "    final_matrix.to_csv(output_path)\n",
    "    print(f\"\\nProcessed matrix saved to: {output_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {FILE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec5f2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class YieldCurveSimulator:\n",
    "    def __init__(self, n_components=3):\n",
    "        \"\"\"\n",
    "        Initializes the simulator using PCA.\n",
    "        \n",
    "        Args:\n",
    "            n_components: Number of PCA components to use. \n",
    "                          3 is standard (Level, Slope, Curvature).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.hist_pc_shocks = None\n",
    "        self.feature_names = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, data_df):\n",
    "        \"\"\"\n",
    "        Fits the PCA model to the historical rate CHANGES.\n",
    "        \n",
    "        Args:\n",
    "            data_df: DataFrame containing historical daily CHANGES in rates.\n",
    "                     (Index should be Date, columns are tenors)\n",
    "        \"\"\"\n",
    "        self.feature_names = data_df.columns\n",
    "        \n",
    "        # 1. Standardize the data (Mean=0, Var=1)\n",
    "        # This is crucial so high-variance rates don't dominate the PCA\n",
    "        scaled_data = self.scaler.fit_transform(data_df)\n",
    "        \n",
    "        # 2. Fit PCA\n",
    "        self.pca.fit(scaled_data)\n",
    "        \n",
    "        # 3. Transform historical data into Principal Components (PC space)\n",
    "        # These are the \"historical shocks\" we will sample from later\n",
    "        self.hist_pc_shocks = self.pca.transform(scaled_data)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Analysis printout\n",
    "        explained_var = np.sum(self.pca.explained_variance_ratio_)\n",
    "        print(f\"Model Fitted. Top {self.n_components} components explain {explained_var:.2%} of variance.\")\n",
    "        print(f\"PC1 (Level): {self.pca.explained_variance_ratio_[0]:.2%}\")\n",
    "        print(f\"PC2 (Slope): {self.pca.explained_variance_ratio_[1]:.2%}\")\n",
    "        if self.n_components > 2:\n",
    "            print(f\"PC3 (Curvature): {self.pca.explained_variance_ratio_[2]:.2%}\")\n",
    "\n",
    "    def simulate_scenarios(self, n_scenarios, n_steps, initial_curve=None, seed=None):\n",
    "        \"\"\"\n",
    "        Generates N scenarios of future yield curves.\n",
    "        \n",
    "        Args:\n",
    "            n_scenarios: Number of independent paths to generate (e.g., 1000)\n",
    "            n_steps: Number of days to simulate per path (e.g., 252 for 1 year)\n",
    "            initial_curve: (Optional) Array of starting rates. If None, starts at 0.0.\n",
    "            seed: Random seed for reproducibility.\n",
    "            \n",
    "        Returns:\n",
    "            scenarios: 3D Array of shape (n_scenarios, n_steps + 1, n_rates)\n",
    "                       containing ABSOLUTE rates.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call .fit() first.\")\n",
    "            \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # --- 1. Bootstrap Resampling ---\n",
    "        # We need (n_scenarios * n_steps) total daily shocks.\n",
    "        # We randomly sample indices from our historical PC shocks.\n",
    "        # This preserves the \"fat tails\" of the real market data.\n",
    "        n_historical_days = self.hist_pc_shocks.shape[0]\n",
    "        random_indices = np.random.randint(0, n_historical_days, size=(n_scenarios, n_steps))\n",
    "        \n",
    "        # Gather the PC shocks corresponding to those indices\n",
    "        # Shape: (n_scenarios, n_steps, n_components)\n",
    "        simulated_pc_shocks = self.hist_pc_shocks[random_indices]\n",
    "\n",
    "        # --- 2. Inverse Transform (Back to Rate Space) ---\n",
    "        # We process this in 2D for efficiency, then reshape back\n",
    "        flat_pc_shocks = simulated_pc_shocks.reshape(-1, self.n_components)\n",
    "        \n",
    "        # Inverse PCA: PC Space -> Scaled Rate Space\n",
    "        flat_scaled_shocks = self.pca.inverse_transform(flat_pc_shocks)\n",
    "        \n",
    "        # Inverse Scaling: Scaled Rate Space -> Actual Rate Changes\n",
    "        flat_real_shocks = self.scaler.inverse_transform(flat_scaled_shocks)\n",
    "        \n",
    "        # Reshape back to (n_scenarios, n_steps, n_rates)\n",
    "        daily_shocks = flat_real_shocks.reshape(n_scenarios, n_steps, len(self.feature_names))\n",
    "\n",
    "        # --- 3. Construct Absolute Curves (Cumulative Sum) ---\n",
    "        # If no initial curve provided, assume starting at 0 (or use last historical point)\n",
    "        if initial_curve is None:\n",
    "            initial_curve = np.zeros(len(self.feature_names))\n",
    "        else:\n",
    "            initial_curve = np.array(initial_curve)\n",
    "\n",
    "        # Create container including t=0\n",
    "        scenarios = np.zeros((n_scenarios, n_steps + 1, len(self.feature_names)))\n",
    "        \n",
    "        # Set t=0\n",
    "        scenarios[:, 0, :] = initial_curve\n",
    "        \n",
    "        # Calculate cumulative path\n",
    "        # path[t] = path[t-1] + shock[t]\n",
    "        cumulative_shocks = np.cumsum(daily_shocks, axis=1)\n",
    "        \n",
    "        # Add cumulative shocks to initial curve\n",
    "        scenarios[:, 1:, :] = cumulative_shocks + initial_curve\n",
    "        \n",
    "        return scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24655c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Loaded. Shape: (8757, 13)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Step B: Fit PCA\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# We use 3 components because they typically explain >95% of yield curve moves.\u001b[39;00m\n\u001b[32m     34\u001b[39m pca = PCA(n_components=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_changes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Step C: Get Historical Shocks in PC Space\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# This transforms our matrix of [Rates] into a matrix of [Drivers]\u001b[39;00m\n\u001b[32m     39\u001b[39m historical_pc_shocks = pca.transform(scaled_changes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:440\u001b[39m, in \u001b[36mPCA.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    424\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[32m    425\u001b[39m \n\u001b[32m    426\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:503\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m     )\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#####\n",
    "#try simpel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import io\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "# I am using the snippet you provided. \n",
    "# To use your actual file, uncomment the line below:\n",
    "# df = pd.read_csv('yield-curve-rates-1990-20241_forward_changes_matrix_2019-02-01_to_2024-12-31_with_months.csv')\n",
    "\n",
    "\n",
    "print(\"1. Data Loaded. Shape:\", df.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PCA MODELING (Training the Generator)\n",
    "# ==========================================\n",
    "# Goal: Find the 3 main \"drivers\" (Principal Components) of yield curve changes.\n",
    "# Why: The 7 rates are highly correlated. We want to model the underlying\n",
    "#      drivers (Level, Slope, Curvature) instead of the 7 rates directly.\n",
    "\n",
    "# Step A: Normalize the data (PCA requires this)\n",
    "# Scales data so mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "scaled_changes = scaler.fit_transform(df)\n",
    "\n",
    "# Step B: Fit PCA\n",
    "# We use 3 components because they typically explain >95% of yield curve moves.\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(scaled_changes)\n",
    "\n",
    "# Step C: Get Historical Shocks in PC Space\n",
    "# This transforms our matrix of [Rates] into a matrix of [Drivers]\n",
    "historical_pc_shocks = pca.transform(scaled_changes)\n",
    "\n",
    "print(\"\\n2. PCA Model Fitted.\")\n",
    "print(f\"   - Explained Variance: {np.sum(pca.explained_variance_ratio_):.2%}\")\n",
    "print(\"   - We have compressed 7 rates into 3 drivers (Level, Slope, Curvature).\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. SIMULATION (Generating Scenarios)\n",
    "# ==========================================\n",
    "# Goal: Create thousands of NEW, plausible future paths for the yield curve.\n",
    "# Method: \"Bootstrapping\" - We pick random days from history and string them together.\n",
    "\n",
    "def simulate_yield_curve(n_scenarios, n_days, start_curve):\n",
    "    \"\"\"\n",
    "    Generates simulated yield curves for ALM training.\n",
    "    \n",
    "    Args:\n",
    "        n_scenarios: How many different futures to simulate (e.g., 1000)\n",
    "        n_days: Length of simulation in days (e.g., 252 for 1 year)\n",
    "        start_curve: The yield curve at t=0 (array of 7 rates)\n",
    "        \n",
    "    Returns:\n",
    "        scenarios: 3D array [Scenario, Day, Rate] with ABSOLUTE rates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Sample random indices from our history\n",
    "    # We need (Scenarios * Days) total random picks\n",
    "    max_idx = len(historical_pc_shocks)\n",
    "    random_indices = np.random.randint(0, max_idx, size=(n_scenarios, n_days))\n",
    "    \n",
    "    # 2. Retrieve the PC shocks for those random days\n",
    "    # Shape: (n_scenarios, n_days, 3)\n",
    "    sim_pc_shocks = historical_pc_shocks[random_indices]\n",
    "    \n",
    "    # 3. Convert PC shocks back to Rate shocks (Inverse PCA)\n",
    "    # Reshape to 2D for the library function, then back to 3D\n",
    "    flat_pc_shocks = sim_pc_shocks.reshape(-1, 3)\n",
    "    flat_scaled_rate_shocks = pca.inverse_transform(flat_pc_shocks)\n",
    "    flat_real_rate_shocks = scaler.inverse_transform(flat_scaled_rate_shocks)\n",
    "    \n",
    "    # Shape: (n_scenarios, n_days, 7)\n",
    "    sim_rate_changes = flat_real_rate_shocks.reshape(n_scenarios, n_days, 7)\n",
    "    \n",
    "    # 4. Construct Absolute Rates (Cumulative Sum)\n",
    "    # We start with the initial curve and add the daily changes\n",
    "    scenarios = np.zeros((n_scenarios, n_days + 1, 7))\n",
    "    scenarios[:, 0, :] = start_curve # Set Day 0\n",
    "    \n",
    "    # Calculate cumulative sum of changes\n",
    "    cumulative_changes = np.cumsum(sim_rate_changes, axis=1)\n",
    "    \n",
    "    # Add to start curve\n",
    "    scenarios[:, 1:, :] = cumulative_changes + start_curve\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "# Define parameters\n",
    "N_SCENARIOS = 50     # Generate 50 potential futures\n",
    "N_DAYS = 252         # 1 Year of trading days\n",
    "# Define a starting yield curve (e.g., current market rates)\n",
    "# [3m, 6m, 1y, 2y, 5y, 10y, 20y] - roughly 3% to 4%\n",
    "current_yield_curve = np.array([0.030, 0.032, 0.034, 0.036, 0.038, 0.042, 0.045])\n",
    "\n",
    "# Run Simulation\n",
    "simulated_data = simulate_yield_curve(N_SCENARIOS, N_DAYS, current_yield_curve)\n",
    "\n",
    "print(f\"\\n3. Simulation Complete.\")\n",
    "print(f\"   - Generated Tensor Shape: {simulated_data.shape} -> (Scenarios, Days, Rates)\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. VISUALIZATION\n",
    "# ==========================================\n",
    "# Plotting the 10-Year Rate (Index 5) across different scenarios\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the first 10 scenarios\n",
    "for i in range(10):\n",
    "    plt.plot(simulated_data[i, :, 5], lw=1, alpha=0.7)\n",
    "\n",
    "plt.title(\"Simulated 10-Year Interest Rate Scenarios (1 Year)\")\n",
    "plt.xlabel(\"Trading Days\")\n",
    "plt.ylabel(\"Interest Rate\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be565ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assuming your DataFrame is named 'df' (from the previous step)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# df = pd.read_csv('your_file_path.csv') \u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Convert the 'Date' column from object (string) to datetime objects\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. Set the 'Date' column as the DataFrame index\u001b[39;00m\n\u001b[32m     10\u001b[39m df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calvi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Date'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
